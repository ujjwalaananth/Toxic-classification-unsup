{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" bag of centroid INSULT .ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"6sdguH4MGDq3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"outputId":"7fb21c0c-0dee-4c9e-a2ff-c032599100d4","executionInfo":{"status":"ok","timestamp":1555666052723,"user_tz":-330,"elapsed":9724,"user":{"displayName":"UJJWALA ANANTHESWARAN (B16EE039)","photoUrl":"https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg","userId":"12449722311299145814"}}},"cell_type":"code","source":["!pip install PyDrive"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting PyDrive\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n","\u001b[K    100% |████████████████████████████████| 993kB 23.8MB/s \n","\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.4)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n","Building wheels for collected packages: PyDrive\n","  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n","Successfully built PyDrive\n","Installing collected packages: PyDrive\n","Successfully installed PyDrive-1.3.1\n"],"name":"stdout"}]},{"metadata":{"id":"-QX1GArMKHLx","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VzaDsiazKHvH","colab_type":"code","colab":{}},"cell_type":"code","source":["download = drive.CreateFile({'id': '1Hydbg7KXOoK9Xf-DPFsoMGv6Whdzhh1F'})\n","download.GetContentFile('train_full.csv')\n","download = drive.CreateFile({'id': '1uo-5rbDJKUSBondu2NnaMC4ygu1TwuW0'})\n","download.GetContentFile('test_full.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4VxkHR9syYYt","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","label_name = 'insult'\n","\n","train_full = pd.read_csv('train_full.csv')\n","test_full = pd.read_csv('test_full.csv')\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zT4-qfFoykDh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c73990d6-a934-49e3-cff3-dc843c189c75","executionInfo":{"status":"ok","timestamp":1555666075583,"user_tz":-330,"elapsed":32573,"user":{"displayName":"UJJWALA ANANTHESWARAN (B16EE039)","photoUrl":"https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg","userId":"12449722311299145814"}}},"cell_type":"code","source":["#####INSULT\n","\n","test1 = test_full[ test_full[label_name]==1 ]\n","test0 = test_full[ test_full[label_name]==0 ]\n","\n","train1 = train_full[ train_full[label_name]==1 ]\n","train0 = train_full[ train_full[label_name]==0 ]\n","\n","test0_new = test0.iloc[0:3000,:]\n","train0_new = train0.iloc[0:5000,:]\n","\n","\n","train_full = pd.concat([train1,train0_new],axis='rows',ignore_index=True).reset_index(drop=True)\n","test_full = pd.concat([test1,test0_new],axis='rows',ignore_index=True).reset_index(drop=True)\n","\n","#shuffling concatenated dataframe:\n","train_full = train_full.sample(frac=1).reset_index(drop=True)\n","test_full = test_full.sample(frac=1).reset_index(drop=True)\n","\n","print(train_full.shape,test_full.shape,list(train_full))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(9864, 9) (5426, 9) ['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'non-toxic', 'comment_text']\n"],"name":"stdout"}]},{"metadata":{"id":"IlIyaymSkNfa","colab_type":"text"},"cell_type":"markdown","source":["Create word vectors"]},{"metadata":{"id":"V-1AX3QbKLKy","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.cluster import KMeans\n","from sklearn.metrics import classification_report"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0Ts38FkhWh-j","colab_type":"code","colab":{}},"cell_type":"code","source":["train = train_full[\"comment_text\"]\n","test = test_full[\"comment_text\"]\n","\n","trainlab = train_full[label_name]\n","testlab = test_full[label_name]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x-SNUrY2hOTW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":799},"outputId":"de747865-36b4-44b5-dee4-be5d1ee5003f","executionInfo":{"status":"ok","timestamp":1555666084313,"user_tz":-330,"elapsed":41297,"user":{"displayName":"UJJWALA ANANTHESWARAN (B16EE039)","photoUrl":"https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg","userId":"12449722311299145814"}}},"cell_type":"code","source":["# string cleaning- already done\n","# Download and load the punkt tokenizer for sentence splitting\n","import nltk.data\n","nltk.download(\"popular\")   \n","tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"],"name":"stdout"}]},{"metadata":{"id":"BagC4SDpmXeU","colab_type":"code","colab":{}},"cell_type":"code","source":["# to split a review into parsed sentences returning a list of sentences (each sentence is list of words)\n","def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n","    review = str(review)\n","    raw_sentences = tokenizer.tokenize(review.strip()) # split para into sentences\n","    sentences = []\n","    for raw_sentence in raw_sentences:\n","        if len(raw_sentence) > 0:\n","            #sentences.append( review_to_wordlist(raw_sentence,remove_stopwords) )\n","            sentences.append(raw_sentence) #bcz already cleaned h\n","    return sentences"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bOAif3xdmZlh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"66a936a5-30d3-459f-c055-8e29d7e4e265","executionInfo":{"status":"ok","timestamp":1555666086613,"user_tz":-330,"elapsed":43595,"user":{"displayName":"UJJWALA ANANTHESWARAN (B16EE039)","photoUrl":"https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg","userId":"12449722311299145814"}}},"cell_type":"code","source":["sentences = []  # Initialize an empty list of sentences\n","\n","print (\"Parsing sentences from training set\")\n","for r in train:\n","  sentences += review_to_sentences(r, tokenizer)\n","\n","print (\"Parsing sentences from testing set\")\n","for r in test:\n","    sentences += review_to_sentences(r, tokenizer)\n","    \n","print(sentences[0:5])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Parsing sentences from training set\n","Parsing sentences from testing set\n","['utc official wikipedia doxa rs homosexuality calomny may', 'sorry must reading something else think needed posting disciplines articles diving ex cycling also remove since one article also team sport rosters templates', 'go fuck traitor thank', 'think kill jews homosexuals handicaped people said adolf hitler', 'warning nothing fc k u fc k get warning nothing fc k u fc k get warning nothing fc k u fc k get warning nothing']\n"],"name":"stdout"}]},{"metadata":{"id":"UFQtBhAKmcFM","colab_type":"code","colab":{}},"cell_type":"code","source":["#training and saving model \n","import logging # for nice output messages\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Wnn-nEcpmeX_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":785},"outputId":"d4e4d1b1-1453-4c98-aaa2-2966f99e3c02","executionInfo":{"status":"ok","timestamp":1555666092657,"user_tz":-330,"elapsed":49635,"user":{"displayName":"UJJWALA ANANTHESWARAN (B16EE039)","photoUrl":"https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg","userId":"12449722311299145814"}}},"cell_type":"code","source":["# Set values for various parameters\n","num_features = 100    # Word vector dimensionality                      \n","div_factor = 1000    # division factor for clusters\n","min_word_count = 40   # Minimum word count                        \n","num_workers = 4       # Number of threads to run in parallel\n","context = 10          # Context window size                                                                                    \n","downsampling = 1e-3   # Downsample setting for frequent words\n","# Initialize and train the model (this will take some time)\n","from gensim.models import word2vec\n","print (\"Training model...\")\n","model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["2019-04-19 09:28:08,963 : WARNING : paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n","2019-04-19 09:28:09,097 : INFO : 'pattern' package not found; tag filters are not available for English\n","2019-04-19 09:28:09,109 : INFO : collecting all words and their counts\n","2019-04-19 09:28:09,110 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n","2019-04-19 09:28:09,111 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"],"name":"stderr"},{"output_type":"stream","text":["Training model...\n"],"name":"stdout"},{"output_type":"stream","text":["2019-04-19 09:28:09,367 : INFO : PROGRESS: at sentence #10000, processed 2082751 words, keeping 27 word types\n","2019-04-19 09:28:09,497 : INFO : collected 27 word types from a corpus of 3150444 raw words and 15290 sentences\n","2019-04-19 09:28:09,498 : INFO : Loading a fresh vocabulary\n","2019-04-19 09:28:09,499 : INFO : effective_min_count=40 retains 27 unique words (100% of original 27, drops 0)\n","2019-04-19 09:28:09,500 : INFO : effective_min_count=40 leaves 3150444 word corpus (100% of original 3150444, drops 0)\n","2019-04-19 09:28:09,501 : INFO : deleting the raw counts dictionary of 27 items\n","2019-04-19 09:28:09,504 : INFO : sample=0.001 downsamples 23 most-common words\n","2019-04-19 09:28:09,506 : INFO : downsampling leaves estimated 543933 word corpus (17.3% of prior 3150444)\n","2019-04-19 09:28:09,507 : INFO : estimated required memory for 27 words and 100 dimensions: 35100 bytes\n","2019-04-19 09:28:09,509 : INFO : resetting layer weights\n","2019-04-19 09:28:09,511 : INFO : training model with 4 workers on 27 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n","2019-04-19 09:28:10,517 : INFO : worker thread finished; awaiting finish of 3 more threads\n","2019-04-19 09:28:10,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2019-04-19 09:28:10,530 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2019-04-19 09:28:10,533 : INFO : EPOCH 1 - PROGRESS: at 100.00% examples, 542545 words/s, in_qsize 0, out_qsize 1\n","2019-04-19 09:28:10,539 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2019-04-19 09:28:10,540 : INFO : EPOCH - 1 : training on 3150444 raw words (544503 effective words) took 1.0s, 538951 effective words/s\n","2019-04-19 09:28:11,523 : INFO : worker thread finished; awaiting finish of 3 more threads\n","2019-04-19 09:28:11,526 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2019-04-19 09:28:11,536 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2019-04-19 09:28:11,542 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2019-04-19 09:28:11,543 : INFO : EPOCH - 2 : training on 3150444 raw words (544566 effective words) took 1.0s, 544920 effective words/s\n","2019-04-19 09:28:12,527 : INFO : worker thread finished; awaiting finish of 3 more threads\n","2019-04-19 09:28:12,533 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2019-04-19 09:28:12,537 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2019-04-19 09:28:12,545 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2019-04-19 09:28:12,547 : INFO : EPOCH - 3 : training on 3150444 raw words (543906 effective words) took 1.0s, 546102 effective words/s\n","2019-04-19 09:28:13,517 : INFO : worker thread finished; awaiting finish of 3 more threads\n","2019-04-19 09:28:13,523 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2019-04-19 09:28:13,526 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2019-04-19 09:28:13,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2019-04-19 09:28:13,533 : INFO : EPOCH - 4 : training on 3150444 raw words (543289 effective words) took 1.0s, 553729 effective words/s\n","2019-04-19 09:28:14,516 : INFO : worker thread finished; awaiting finish of 3 more threads\n","2019-04-19 09:28:14,518 : INFO : worker thread finished; awaiting finish of 2 more threads\n","2019-04-19 09:28:14,520 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2019-04-19 09:28:14,529 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2019-04-19 09:28:14,530 : INFO : EPOCH - 5 : training on 3150444 raw words (544623 effective words) took 1.0s, 548871 effective words/s\n","2019-04-19 09:28:14,531 : INFO : training on a 15752220 raw words (2720887 effective words) took 5.0s, 542148 effective words/s\n"],"name":"stderr"}]},{"metadata":{"id":"lU_pvaavn0B5","colab_type":"text"},"cell_type":"markdown","source":["Converting review to word vectors"]},{"metadata":{"id":"y9Jt87aFn8Iu","colab_type":"code","colab":{}},"cell_type":"code","source":["def makeFeatureVec(words, model, num_features):\n","    # Function to average all of the word vectors in a given\n","    # paragraph\n","    #\n","    # Pre-initialize an empty numpy array (for speed)\n","    featureVec = np.zeros((num_features,),dtype=\"float32\")\n","    #\n","    nwords = 0.\n","    # \n","    # Index2word is a list that contains the names of the words in \n","    # the model's vocabulary. Convert it to a set, for speed \n","    index2word_set = set(model.wv.index2word)\n","    #\n","    # Loop over each word in the review and, if it is in the model's\n","    # vocaublary, add its feature vector to the total\n","    for word in words:\n","        if word in index2word_set: \n","            nwords = nwords + 1.\n","            featureVec = np.add(featureVec,model[word])\n","    # \n","    # Divide the result by the number of words to get the average\n","    featureVec = np.divide(featureVec,nwords)\n","    return featureVec\n","\n","\n","def getAvgFeatureVecs(reviews, model, num_features):\n","    # Given a set of reviews (each one a list of words), calculate \n","    # the average feature vector for each one and return a 2D numpy array \n","    # \n","    # Initialize a counter\n","    counter = 0.\n","    # \n","    # Preallocate a 2D numpy array, for speed\n","    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n","    # \n","    # Loop through the reviews\n","    for review in reviews:\n","       #\n","       # Print a status message every 1000th review\n","       if counter%1000. == 0.:\n","           print (\"Review %d of %d\" % (counter, len(reviews)))\n","       # \n","       # Call the function (defined above) that makes average feature vectors\n","       reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, num_features)\n","       print(reviewFeatureVecs)\n","       #\n","       # Increment the counter\n","       counter = counter + 1.\n","    return reviewFeatureVecs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nbkxDQVDn-vf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3379416,"output_embedded_package_id":"1oARHJkwkH6RxmQywjrUSbfGDQXeOb_DG"},"outputId":"b7585466-8d5f-47c8-d867-4240a1803794","executionInfo":{"status":"ok","timestamp":1555666123539,"user_tz":-330,"elapsed":80515,"user":{"displayName":"UJJWALA ANANTHESWARAN (B16EE039)","photoUrl":"https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg","userId":"12449722311299145814"}}},"cell_type":"code","source":["trainDataVecs = getAvgFeatureVecs( train, model, num_features )\n","testDataVecs = getAvgFeatureVecs( test, model, num_features )\n"],"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"metadata":{"id":"7gRprQcCoBy4","colab_type":"text"},"cell_type":"markdown","source":["Clustering these vectors"]},{"metadata":{"id":"Ps_RqNHVZIc-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"76854cb6-49ad-4430-f981-a08435c4dc80","executionInfo":{"status":"ok","timestamp":1555666125524,"user_tz":-330,"elapsed":82500,"user":{"displayName":"UJJWALA ANANTHESWARAN (B16EE039)","photoUrl":"https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg","userId":"12449722311299145814"}}},"cell_type":"code","source":["type(trainDataVecs)\n","print(trainDataVecs.shape)\n","tot = len(trainDataVecs)+len(testDataVecs)\n","print(tot)\n","\n","arr = np.zeros((tot, num_features))\n","\n","for col in range(0,num_features):\n","  for row in range(0, len(trainDataVecs)):\n","    arr[row] = np.array(trainDataVecs[row])\n","  \n","  \n","print(arr.shape)\n","l1 = len(trainDataVecs)\n","l2 = len(testDataVecs)\n","  \n","  \n","for col in range(0,num_features):\n","  for row in range(0, len(testDataVecs)):\n","    arr[l1 + row] = np.array(testDataVecs[row])\n","\n","\n","#word_vectors = np.concatenate(trainDataVecs,testDataVecs)\n","#print(word_vectors.shape)\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["(9864, 100)\n","15290\n","(15290, 100)\n"],"name":"stdout"}]},{"metadata":{"id":"Wnb2Es5WdXLr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"29b5bce4-48b3-4d4d-dca3-a601e48f2d03","executionInfo":{"status":"ok","timestamp":1555666125526,"user_tz":-330,"elapsed":82501,"user":{"displayName":"UJJWALA ANANTHESWARAN (B16EE039)","photoUrl":"https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg","userId":"12449722311299145814"}}},"cell_type":"code","source":["word_vectors = np.matrix(arr)\n","print(word_vectors.shape)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["(15290, 100)\n"],"name":"stdout"}]},{"metadata":{"id":"ollwv_xYoEBH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"596303c4-22f5-4f6f-f114-4a3b903c42b2","executionInfo":{"status":"ok","timestamp":1555666160330,"user_tz":-330,"elapsed":117304,"user":{"displayName":"UJJWALA ANANTHESWARAN (B16EE039)","photoUrl":"https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg","userId":"12449722311299145814"}}},"cell_type":"code","source":["#word_vectors = pd.concat([trainDataVecs,testDataVecs])\n","#num_clusters = int(len(word_vectors) / div_factor)\n","#from sklearn.cluster import DBSCAN\n","\n","\n","num_clusters = 50\n","\n","# Initalize a k-means object and use it to extract centroids\n","import time\n","\n","start = time.time() # Start time\n","\n","\n","clusterer = KMeans( n_clusters = num_clusters, max_iter=500,  n_init=20)\n","\n","idx = clusterer.fit_predict( word_vectors )\n","\n","\n","# Get the end time and print how long the process took\n","end = time.time()\n","elapsed = end - start\n","print (\"Time taken for K Means clustering: \", elapsed, \"seconds.\")\n","print (idx)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Time taken for K Means clustering:  34.752663373947144 seconds.\n","[17 47 35 ...  8 12 17]\n"],"name":"stdout"}]},{"metadata":{"id":"E_fpP0lVeJ7h","colab_type":"text"},"cell_type":"markdown","source":["BAG OF CENTROIDS\n"]},{"metadata":{"id":"Qnxned4xZeW9","colab_type":"code","colab":{}},"cell_type":"code","source":["# Create a Word / Index dictionary, mapping each vocabulary word to\n","# a cluster number                                                                                            \n","word_centroid_map = dict(zip( model.wv.index2word, idx ))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aZZWznxpZeUe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2567},"outputId":"67b4203e-4143-41b5-bb7d-ca85b5e30c92","executionInfo":{"status":"ok","timestamp":1555666160333,"user_tz":-330,"elapsed":117304,"user":{"displayName":"UJJWALA ANANTHESWARAN (B16EE039)","photoUrl":"https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg","userId":"12449722311299145814"}}},"cell_type":"code","source":["# For the first 10 clusters\n","for cluster in range(0,50):\n","    #\n","    # Print the cluster number  \n","    print (\"\\nCluster %d\" % cluster)\n","    #\n","    # Find all of the words for that cluster number, and print them out\n","    words = []\n","    for i in range(0,len(word_centroid_map.values())):\n","        if( list(word_centroid_map.values())[i] == cluster ):\n","            words.append(list(word_centroid_map.keys())[i])\n","    print (words)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["\n","Cluster 0\n","[]\n","\n","Cluster 1\n","['j']\n","\n","Cluster 2\n","[]\n","\n","Cluster 3\n","[]\n","\n","Cluster 4\n","[]\n","\n","Cluster 5\n","['b']\n","\n","Cluster 6\n","[]\n","\n","Cluster 7\n","['m']\n","\n","Cluster 8\n","['q']\n","\n","Cluster 9\n","[]\n","\n","Cluster 10\n","[]\n","\n","Cluster 11\n","[]\n","\n","Cluster 12\n","['w']\n","\n","Cluster 13\n","[]\n","\n","Cluster 14\n","[]\n","\n","Cluster 15\n","[]\n","\n","Cluster 16\n","[]\n","\n","Cluster 17\n","[' ']\n","\n","Cluster 18\n","[]\n","\n","Cluster 19\n","[]\n","\n","Cluster 20\n","[]\n","\n","Cluster 21\n","[]\n","\n","Cluster 22\n","['u', 'k', 'f']\n","\n","Cluster 23\n","['l']\n","\n","Cluster 24\n","[]\n","\n","Cluster 25\n","['a', 'h']\n","\n","Cluster 26\n","[]\n","\n","Cluster 27\n","['d', 'z']\n","\n","Cluster 28\n","[]\n","\n","Cluster 29\n","[]\n","\n","Cluster 30\n","[]\n","\n","Cluster 31\n","['o', 'r']\n","\n","Cluster 32\n","['t', 'p']\n","\n","Cluster 33\n","[]\n","\n","Cluster 34\n","[]\n","\n","Cluster 35\n","['i']\n","\n","Cluster 36\n","['n', 'v']\n","\n","Cluster 37\n","[]\n","\n","Cluster 38\n","['c']\n","\n","Cluster 39\n","[]\n","\n","Cluster 40\n","[]\n","\n","Cluster 41\n","[]\n","\n","Cluster 42\n","[]\n","\n","Cluster 43\n","['y']\n","\n","Cluster 44\n","[]\n","\n","Cluster 45\n","[]\n","\n","Cluster 46\n","[]\n","\n","Cluster 47\n","['e', 'g', 'x']\n","\n","Cluster 48\n","[]\n","\n","Cluster 49\n","['s']\n"],"name":"stdout"}]},{"metadata":{"id":"r9srmZOMZeRe","colab_type":"code","colab":{}},"cell_type":"code","source":["def create_bag_of_centroids( wordlist, word_centroid_map ):\n","    #\n","    # The number of clusters is equal to the highest cluster index\n","    # in the word / centroid map\n","    num_centroids = num_clusters\n","    #\n","    # Pre-allocate the bag of centroids vector (for speed)\n","    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n","    #\n","    #print(len(bag_of_centroids))\n","    # Loop over the words in the review. If the word is in the vocabulary,\n","    # find which cluster it belongs to, and increment that cluster count \n","    # by one\n","    for word in wordlist:\n","        if word in word_centroid_map:\n","            index = word_centroid_map[word]\n","            bag_of_centroids[index] += 1\n","    #\n","    # Return the \"bag of centroids\"\n","    #print(len(bag_of_centroids))\n","    return bag_of_centroids"],"execution_count":0,"outputs":[]},{"metadata":{"id":"W0FaVB-mZePE","colab_type":"code","colab":{}},"cell_type":"code","source":["# Pre-allocate an array for the training set bags of centroids (for speed)\n","train_centroids = np.zeros( (train.size, num_clusters), dtype=\"float32\" )\n","\n","# Transform the training set reviews into bags of centroids\n","counter = 0\n","for review in train:\n","    train_centroids[counter][:] =  create_bag_of_centroids( review, word_centroid_map ) \n","    counter = counter + 1\n","\n","# Repeat for test reviews \n","test_centroids = np.zeros(( test.size, num_clusters), dtype=\"float32\" )\n","\n","counter = 0\n","for review in test:\n","    test_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n","    counter += 1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K_Ur3yH6wRg1","colab_type":"text"},"cell_type":"markdown","source":["SVC poly"]},{"metadata":{"id":"AbPL-w2-pNoO","colab_type":"code","outputId":"cf84ff9c-d309-4856-d41e-659069ad7f4e","colab":{"base_uri":"https://localhost:8080/","height":88}},"cell_type":"code","source":["from sklearn.svm import SVC\n","# Fit a random forest and extract predictions \n","forest = SVC(kernel='poly')\n","\n","# Fitting the forest may take a few minutes\n","print (\"Fitting a random forest to labeled training data...\")\n","forest = forest.fit(train_centroids,train_full[label_name])\n","result = forest.predict(test_centroids)\n","\n","# Write the test results \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Fitting a random forest to labeled training data...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n"],"name":"stderr"}]},{"metadata":{"id":"Ipgnf-HqwW-F","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.metrics import classification_report\n","#test data\n","print(classification_report(testlab, result))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ac-PHdzvwTX0","colab_type":"text"},"cell_type":"markdown","source":["Random forest"]},{"metadata":{"id":"GxGINFB5waEj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"4137389f-4249-489a-f116-96a3865a3e6d","executionInfo":{"status":"ok","timestamp":1555666314584,"user_tz":-330,"elapsed":1361,"user":{"displayName":"UJJWALA ANANTHESWARAN (B16EE039)","photoUrl":"https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg","userId":"12449722311299145814"}}},"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","# Fit a random forest and extract predictions \n","forest = RandomForestClassifier()\n","# Fitting the forest may take a few minutes\n","print (\"Fitting a random forest to labeled training data...\")\n","forest = forest.fit(train_centroids,train_full[label_name])\n","result = forest.predict(test_centroids)\n","\n","# Write the test results \n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Fitting a random forest to labeled training data...\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n","  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"],"name":"stderr"}]},{"metadata":{"id":"K5shSxnZpTrw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"2d69787d-465c-4465-b474-ac3bbdd11e22","executionInfo":{"status":"ok","timestamp":1555666314590,"user_tz":-330,"elapsed":1352,"user":{"displayName":"UJJWALA ANANTHESWARAN (B16EE039)","photoUrl":"https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg","userId":"12449722311299145814"}}},"cell_type":"code","source":["from sklearn.metrics import classification_report\n","#test data\n","print(classification_report(testlab, result))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.74      0.71      3000\n","           1       0.64      0.57      0.60      2426\n","\n","   micro avg       0.66      0.66      0.66      5426\n","   macro avg       0.66      0.66      0.66      5426\n","weighted avg       0.66      0.66      0.66      5426\n","\n"],"name":"stdout"}]}]}