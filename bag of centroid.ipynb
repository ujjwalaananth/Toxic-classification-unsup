{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4721,
     "status": "ok",
     "timestamp": 1555666877829,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "6sdguH4MGDq3",
    "outputId": "952ce2bc-761f-4a24-91a1-1d524459e3f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
      "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
      "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.4)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.11.3)\n",
      "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.11.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QX1GArMKHLx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5980,
     "status": "ok",
     "timestamp": 1555666879119,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "VzaDsiazKHvH",
    "outputId": "da2f3da3-1a7f-4bca-e2af-14fcfdcebdad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-19 09:41:19,934 : WARNING : file_cache is unavailable when using oauth2client >= 4.0.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ModuleNotFoundError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n",
      "2019-04-19 09:41:19,939 : INFO : URL being requested: GET https://www.googleapis.com/discovery/v1/apis/drive/v2/rest\n",
      "2019-04-19 09:41:19,941 : INFO : Attempting refresh to obtain initial access_token\n",
      "2019-04-19 09:41:19,944 : INFO : Refreshing access_token\n",
      "2019-04-19 09:41:20,151 : INFO : URL being requested: GET https://www.googleapis.com/drive/v2/files/1Hydbg7KXOoK9Xf-DPFsoMGv6Whdzhh1F?alt=json\n",
      "2019-04-19 09:41:20,818 : INFO : URL being requested: GET https://www.googleapis.com/drive/v2/files/1uo-5rbDJKUSBondu2NnaMC4ygu1TwuW0?alt=json\n"
     ]
    }
   ],
   "source": [
    "download = drive.CreateFile({'id': '1Hydbg7KXOoK9Xf-DPFsoMGv6Whdzhh1F'})\n",
    "download.GetContentFile('train_full.csv')\n",
    "download = drive.CreateFile({'id': '1uo-5rbDJKUSBondu2NnaMC4ygu1TwuW0'})\n",
    "download.GetContentFile('test_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VxkHR9syYYt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "label_name = 'non-toxic'\n",
    "\n",
    "train_full = pd.read_csv('train_full.csv')\n",
    "test_full = pd.read_csv('test_full.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29405,
     "status": "ok",
     "timestamp": 1555666534354,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "tsLdcA5uemOQ",
    "outputId": "d258c23f-0535-4bd7-ca31-f4bef4a11dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23993, 9) (6998, 9)\n"
     ]
    }
   ],
   "source": [
    "more_rows = test_full.iloc[0:4000,:]\n",
    "test_full = test_full.drop(test_full.index[0:4000])\n",
    "\n",
    "train_full = pd.concat([train_full,more_rows],axis='rows',ignore_index=True).reset_index(drop=True)\n",
    "print(train_full.shape,test_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlIyaymSkNfa"
   },
   "source": [
    "Create word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-1AX3QbKLKy"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Ts38FkhWh-j"
   },
   "outputs": [],
   "source": [
    "train = train_full[\"comment_text\"]\n",
    "test = test_full[\"comment_text\"]\n",
    "\n",
    "trainlab = train_full[label_name]\n",
    "testlab = test_full[label_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1947,
     "status": "ok",
     "timestamp": 1555666882474,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "x-SNUrY2hOTW",
    "outputId": "6dee664c-13b1-48fb-a15f-1311d1195e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "# string cleaning- already done\n",
    "# Download and load the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "nltk.download(\"popular\")   \n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BagC4SDpmXeU"
   },
   "outputs": [],
   "source": [
    "# to split a review into parsed sentences returning a list of sentences (each sentence is list of words)\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    review = str(review)\n",
    "    raw_sentences = tokenizer.tokenize(review.strip()) # split para into sentences\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            #sentences.append( review_to_wordlist(raw_sentence,remove_stopwords) )\n",
    "            sentences.append(raw_sentence) #bcz already cleaned h\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3808,
     "status": "ok",
     "timestamp": 1555666884354,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "bOAif3xdmZlh",
    "outputId": "f6d59878-8e87-47af-d390-8b00b26fa0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "Parsing sentences from testing set\n",
      "['guys fuckin crazy stupid shit lick pimply fat ugly bum crazy gays faggots male lesbian know gonna delete stupid block thing care single weirdest boobs okay hate bitches', 'guys victimizing ethnicity blatant racism', 'delete harry walther article worried deletion walther one voices preaching truth jesus message', 'think give shit', 'disagree fact used single party government largely irrelvance important thing still constitutional monarchy parliamentary system allows coalitions majority rule changed say currently coalition whatever party means position deputy prime minister included alongside heads state government false mentioning nick clegg deputy prime minister article around place david cameron currently described prime minister put clegg alongside queen prime minister basis coalition government right job constitutionally defined prime minister second command cameron die resign suddenly clegg would become prime minister would completely agree clegg may deputy prime minister deputy head government put alongside head state head government']\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print (\"Parsing sentences from training set\")\n",
    "for r in train:\n",
    "  sentences += review_to_sentences(r, tokenizer)\n",
    "\n",
    "print (\"Parsing sentences from testing set\")\n",
    "for r in test:\n",
    "    sentences += review_to_sentences(r, tokenizer)\n",
    "    \n",
    "print(sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UFQtBhAKmcFM"
   },
   "outputs": [],
   "source": [
    "#training and saving model \n",
    "import logging # for nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15665,
     "status": "ok",
     "timestamp": 1555666896229,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "Wnn-nEcpmeX_",
    "outputId": "845e11c5-d9e5-4a5a-f31a-9d360f3ec268"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-19 09:41:26,238 : INFO : collecting all words and their counts\n",
      "2019-04-19 09:41:26,240 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2019-04-19 09:41:26,242 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-19 09:41:26,517 : INFO : PROGRESS: at sentence #10000, processed 2216528 words, keeping 27 word types\n",
      "2019-04-19 09:41:26,796 : INFO : PROGRESS: at sentence #20000, processed 4506219 words, keeping 27 word types\n",
      "2019-04-19 09:41:27,072 : INFO : PROGRESS: at sentence #30000, processed 6732145 words, keeping 27 word types\n",
      "2019-04-19 09:41:27,100 : INFO : collected 27 word types from a corpus of 6929356 raw words and 30991 sentences\n",
      "2019-04-19 09:41:27,101 : INFO : Loading a fresh vocabulary\n",
      "2019-04-19 09:41:27,102 : INFO : effective_min_count=40 retains 27 unique words (100% of original 27, drops 0)\n",
      "2019-04-19 09:41:27,103 : INFO : effective_min_count=40 leaves 6929356 word corpus (100% of original 6929356, drops 0)\n",
      "2019-04-19 09:41:27,104 : INFO : deleting the raw counts dictionary of 27 items\n",
      "2019-04-19 09:41:27,105 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2019-04-19 09:41:27,107 : INFO : downsampling leaves estimated 1194589 word corpus (17.2% of prior 6929356)\n",
      "2019-04-19 09:41:27,109 : INFO : estimated required memory for 27 words and 100 dimensions: 35100 bytes\n",
      "2019-04-19 09:41:27,111 : INFO : resetting layer weights\n",
      "2019-04-19 09:41:27,112 : INFO : training model with 4 workers on 27 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-04-19 09:41:28,135 : INFO : EPOCH 1 - PROGRESS: at 44.11% examples, 535272 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-19 09:41:29,136 : INFO : EPOCH 1 - PROGRESS: at 88.88% examples, 534584 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-19 09:41:29,365 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-19 09:41:29,371 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-19 09:41:29,378 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-19 09:41:29,383 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-19 09:41:29,384 : INFO : EPOCH - 1 : training on 6929356 raw words (1195912 effective words) took 2.3s, 531348 effective words/s\n",
      "2019-04-19 09:41:30,394 : INFO : EPOCH 2 - PROGRESS: at 44.95% examples, 544953 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-19 09:41:31,397 : INFO : EPOCH 2 - PROGRESS: at 90.41% examples, 542974 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-19 09:41:31,590 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-19 09:41:31,594 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-19 09:41:31,594 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-19 09:41:31,599 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-19 09:41:31,601 : INFO : EPOCH - 2 : training on 6929356 raw words (1195205 effective words) took 2.2s, 540944 effective words/s\n",
      "2019-04-19 09:41:32,614 : INFO : EPOCH 3 - PROGRESS: at 44.78% examples, 541779 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-19 09:41:33,616 : INFO : EPOCH 3 - PROGRESS: at 90.21% examples, 540911 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-19 09:41:33,818 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-19 09:41:33,825 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-19 09:41:33,829 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-19 09:41:33,830 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-19 09:41:33,831 : INFO : EPOCH - 3 : training on 6929356 raw words (1194037 effective words) took 2.2s, 537912 effective words/s\n",
      "2019-04-19 09:41:34,840 : INFO : EPOCH 4 - PROGRESS: at 44.24% examples, 537076 words/s, in_qsize 8, out_qsize 1\n",
      "2019-04-19 09:41:35,844 : INFO : EPOCH 4 - PROGRESS: at 89.55% examples, 536626 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-19 09:41:36,052 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-19 09:41:36,064 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-19 09:41:36,065 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-19 09:41:36,066 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-19 09:41:36,071 : INFO : EPOCH - 4 : training on 6929356 raw words (1193141 effective words) took 2.2s, 534586 effective words/s\n",
      "2019-04-19 09:41:37,076 : INFO : EPOCH 5 - PROGRESS: at 45.44% examples, 551005 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-19 09:41:38,078 : INFO : EPOCH 5 - PROGRESS: at 92.04% examples, 551405 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-19 09:41:38,233 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-19 09:41:38,241 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-19 09:41:38,243 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-19 09:41:38,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-19 09:41:38,255 : INFO : EPOCH - 5 : training on 6929356 raw words (1193786 effective words) took 2.2s, 547866 effective words/s\n",
      "2019-04-19 09:41:38,256 : INFO : training on a 34646780 raw words (5972081 effective words) took 11.1s, 536013 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "div_factor = 1000    # division factor for clusters\n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lU_pvaavn0B5"
   },
   "source": [
    "Converting review to word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9Jt87aFn8Iu"
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000. == 0.:\n",
    "           print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, num_features)\n",
    "       print(reviewFeatureVecs)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6849592,
     "output_embedded_package_id": "1qGMY20Ia6u4Cd0uVH3p01_ZL71vo1I-P"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 81395,
     "status": "ok",
     "timestamp": 1555666961977,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "nbkxDQVDn-vf",
    "outputId": "bc3481a9-8ffb-422d-f83b-c2e1dd29f6ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainDataVecs = getAvgFeatureVecs( train, model, num_features )\n",
    "testDataVecs = getAvgFeatureVecs( test, model, num_features )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gRprQcCoBy4"
   },
   "source": [
    "Clustering these vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 85425,
     "status": "ok",
     "timestamp": 1555666966020,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "Ps_RqNHVZIc-",
    "outputId": "3431b7b7-7611-43a1-e51e-d79de520b17c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19993, 100)\n",
      "30991\n",
      "(30991, 100)\n"
     ]
    }
   ],
   "source": [
    "type(trainDataVecs)\n",
    "print(trainDataVecs.shape)\n",
    "tot = len(trainDataVecs)+len(testDataVecs)\n",
    "print(tot)\n",
    "\n",
    "arr = np.zeros((tot, num_features))\n",
    "\n",
    "for col in range(0,num_features):\n",
    "  for row in range(0, len(trainDataVecs)):\n",
    "    arr[row] = np.array(trainDataVecs[row])\n",
    "  \n",
    "  \n",
    "print(arr.shape)\n",
    "l1 = len(trainDataVecs)\n",
    "l2 = len(testDataVecs)\n",
    "  \n",
    "  \n",
    "for col in range(0,num_features):\n",
    "  for row in range(0, len(testDataVecs)):\n",
    "    arr[l1 + row] = np.array(testDataVecs[row])\n",
    "\n",
    "\n",
    "#word_vectors = np.concatenate(trainDataVecs,testDataVecs)\n",
    "#print(word_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 85421,
     "status": "ok",
     "timestamp": 1555666966021,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "Wnb2Es5WdXLr",
    "outputId": "a19f9871-f05d-4fcf-ae79-6651f8e75201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30991, 100)\n"
     ]
    }
   ],
   "source": [
    "word_vectors = np.matrix(arr)\n",
    "print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 183247,
     "status": "ok",
     "timestamp": 1555667063857,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "ollwv_xYoEBH",
    "outputId": "306ca176-cc8c-4315-9d52-83860a223dce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  97.80170440673828 seconds.\n",
      "[10 26 34 ... 18 17 43]\n"
     ]
    }
   ],
   "source": [
    "#word_vectors = pd.concat([trainDataVecs,testDataVecs])\n",
    "#num_clusters = int(len(word_vectors) / div_factor)\n",
    "#from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "num_clusters = 50\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "\n",
    "clusterer = KMeans( n_clusters = num_clusters, max_iter=500,  n_init=20)\n",
    "\n",
    "idx = clusterer.fit_predict( word_vectors )\n",
    "\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print (\"Time taken for K Means clustering: \", elapsed, \"seconds.\")\n",
    "print (idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_fpP0lVeJ7h"
   },
   "source": [
    "BAG OF CENTROIDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qnxned4xZeW9"
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2567
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 183238,
     "status": "ok",
     "timestamp": 1555667063860,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "aZZWznxpZeUe",
    "outputId": "753e8dd6-d7f3-4a62-95d8-98be237be82c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "[]\n",
      "\n",
      "Cluster 1\n",
      "['s', 'l', 'u', 'q']\n",
      "\n",
      "Cluster 2\n",
      "['a']\n",
      "\n",
      "Cluster 3\n",
      "['p', 'h', 'z']\n",
      "\n",
      "Cluster 4\n",
      "[]\n",
      "\n",
      "Cluster 5\n",
      "['m']\n",
      "\n",
      "Cluster 6\n",
      "[]\n",
      "\n",
      "Cluster 7\n",
      "[]\n",
      "\n",
      "Cluster 8\n",
      "[]\n",
      "\n",
      "Cluster 9\n",
      "[]\n",
      "\n",
      "Cluster 10\n",
      "[' ', 'c']\n",
      "\n",
      "Cluster 11\n",
      "[]\n",
      "\n",
      "Cluster 12\n",
      "[]\n",
      "\n",
      "Cluster 13\n",
      "[]\n",
      "\n",
      "Cluster 14\n",
      "[]\n",
      "\n",
      "Cluster 15\n",
      "[]\n",
      "\n",
      "Cluster 16\n",
      "['n']\n",
      "\n",
      "Cluster 17\n",
      "[]\n",
      "\n",
      "Cluster 18\n",
      "[]\n",
      "\n",
      "Cluster 19\n",
      "['r']\n",
      "\n",
      "Cluster 20\n",
      "['y']\n",
      "\n",
      "Cluster 21\n",
      "[]\n",
      "\n",
      "Cluster 22\n",
      "[]\n",
      "\n",
      "Cluster 23\n",
      "[]\n",
      "\n",
      "Cluster 24\n",
      "['x']\n",
      "\n",
      "Cluster 25\n",
      "[]\n",
      "\n",
      "Cluster 26\n",
      "['e', 'o', 'd', 'j']\n",
      "\n",
      "Cluster 27\n",
      "[]\n",
      "\n",
      "Cluster 28\n",
      "[]\n",
      "\n",
      "Cluster 29\n",
      "['b']\n",
      "\n",
      "Cluster 30\n",
      "[]\n",
      "\n",
      "Cluster 31\n",
      "[]\n",
      "\n",
      "Cluster 32\n",
      "[]\n",
      "\n",
      "Cluster 33\n",
      "[]\n",
      "\n",
      "Cluster 34\n",
      "['i', 'f']\n",
      "\n",
      "Cluster 35\n",
      "[]\n",
      "\n",
      "Cluster 36\n",
      "[]\n",
      "\n",
      "Cluster 37\n",
      "[]\n",
      "\n",
      "Cluster 38\n",
      "[]\n",
      "\n",
      "Cluster 39\n",
      "['v']\n",
      "\n",
      "Cluster 40\n",
      "[]\n",
      "\n",
      "Cluster 41\n",
      "['t', 'g', 'k']\n",
      "\n",
      "Cluster 42\n",
      "[]\n",
      "\n",
      "Cluster 43\n",
      "[]\n",
      "\n",
      "Cluster 44\n",
      "[]\n",
      "\n",
      "Cluster 45\n",
      "[]\n",
      "\n",
      "Cluster 46\n",
      "['w']\n",
      "\n",
      "Cluster 47\n",
      "[]\n",
      "\n",
      "Cluster 48\n",
      "[]\n",
      "\n",
      "Cluster 49\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# For the first 10 clusters\n",
    "for cluster in range(0,50):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print (\"\\nCluster %d\" % cluster)\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    for i in range(0,len(word_centroid_map.values())):\n",
    "        if( list(word_centroid_map.values())[i] == cluster ):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9srmZOMZeRe"
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = num_clusters\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    #print(len(bag_of_centroids))\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    #print(len(bag_of_centroids))\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0FaVB-mZePE"
   },
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train.size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in train:\n",
    "    train_centroids[counter][:] =  create_bag_of_centroids( review, word_centroid_map ) \n",
    "    counter = counter + 1\n",
    "\n",
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test.size, num_clusters), dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in test:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gM5jOK4q8Zil"
   },
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 204402,
     "status": "ok",
     "timestamp": 1555667085044,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "zIb81jE48dEs",
    "outputId": "fcdda562-748f-4a1d-d0c5-e38887fbb86e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Fit a random forest and extract predictions \n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print (\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train_full[\"non-toxic\"])\n",
    "result = forest.predict(test_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 204399,
     "status": "ok",
     "timestamp": 1555667085046,
     "user": {
      "displayName": "UJJWALA ANANTHESWARAN (B16EE039)",
      "photoUrl": "https://lh4.googleusercontent.com/-zwPghDnGkDc/AAAAAAAAAAI/AAAAAAAAAqo/Q8kjkBUR8xY/s64/photo.jpg",
      "userId": "12449722311299145814"
     },
     "user_tz": -330
    },
    "id": "xdeXOJ-J8nAA",
    "outputId": "5774e488-dfed-40a4-feaf-6f606b23fcd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.71      0.63      5000\n",
      "           1       0.69      0.55      0.62      5998\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     10998\n",
      "   macro avg       0.63      0.63      0.62     10998\n",
      "weighted avg       0.64      0.62      0.62     10998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#test data\n",
    "print(classification_report(testlab, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XVMtJH4g8U9W"
   },
   "source": [
    "SVM poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "JzE54sLzw0KZ",
    "outputId": "32bcc898-de5e-4137-b5a9-10f6c9d8ec61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Fit a random forest and extract predictions \n",
    "forest = SVC(kernel = 'poly')\n",
    "\n",
    "# Fitting the forest may take a few minutes\n",
    "print (\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(train_centroids,train_full[\"non-toxic\"])\n",
    "result = forest.predict(test_centroids)\n",
    "\n",
    "# Write the test results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "648fLjY6w0EY"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#test data\n",
    "print(classification_report(testlab, result))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bag of centroid.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
